{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Synthetic_same_dim_pretrain_angles_radius_best.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"86dBMxv4p4Lm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597705567294,"user_tz":-60,"elapsed":688,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}}},"source":["experiment_name = 'knn_pretrain_angles_radius_best_base'"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWCVoN7oDpaV","colab_type":"text"},"source":["# Colab settings"]},{"cell_type":"code","metadata":{"id":"l-oWypvjDogQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597705569296,"user_tz":-60,"elapsed":634,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}},"outputId":"9981715b-f508-44d6-b206-61ed5c74bc40"},"source":["#Mount gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Z3oEsybDORw","colab_type":"text"},"source":["# Imports and Settings"]},{"cell_type":"code","metadata":{"id":"MES2en4ADOR7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597705570612,"user_tz":-60,"elapsed":677,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}}},"source":["#Paths\n","import os\n","from pathlib import Path\n","#fp_repo = \"/Users/roseline/Docs/UCL/Projects/B.Roads/B.Roads code/Fork/unsupervised-alignment-team-master\"\n","fp_repo = '/content/gdrive/My Drive/unsupervised-alignment-team-master'\n","fp_intersect = fp_repo / Path('python','assets','intersect')\n","fp_save = fp_repo / Path('save')\n","os.chdir(fp_repo / Path('python'))"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFppTe0uDORx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597705571047,"user_tz":-60,"elapsed":572,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}}},"source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import scipy as sp\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import uuid\n","import time\n","import itertools\n","#Custom modules\n","import embeddings as emb \n","import utils \n","import models\n","import losses\n","import train\n","import knn\n","#KERAS\n","import keras\n","import tensorflow as tf\n","#New\n","from scipy.spatial import distance\n","import scipy.interpolate as interpolate\n","from scipy.integrate import quad\n","import matplotlib.pyplot as plt"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRtgp2pFIbjS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597705571658,"user_tz":-60,"elapsed":660,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}},"outputId":"c47f2e4a-6a3f-4283-a697-b4fd9a5cb8e6"},"source":["def gpu_avail():\n","  is_avail = True\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    is_avail = False\n","  return is_avail  \n","print(gpu_avail())\n","# tf.debugging.set_log_device_placement(True)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4k4DldSkDOR9","colab_type":"text"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"hrwiuQYFDOR-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597705572914,"user_tz":-60,"elapsed":700,"user":{"displayName":"Roseline","photoUrl":"","userId":"06567256556577965216"}}},"source":["#===PARAMETERS===\\\n","override = True\n","sd = 56 #This should make the subsequend rd seed generation reproducible\n","tf.random.set_seed(sd)\n","np.random.seed(sd)\n","#=Synthetic data\n","n_systems = 2\n","n_concepts = 200\n","noise = 0.001\n","emb_dim = 2\n","n_epicentres = 1\n","linearsep = 1\n","#=Neural net\n","n_runs = 10\n","max_restart = 110 #10 last restarts to fine tune best\n","max_epoch = 30\n","batch_size = np.minimum(100, n_concepts) #64/100\n","#=losses\n","gmm_scale = 0.01\n","loss_distr_scale = 1.0\n","loss_cycle_scale = 10000.0\n","lr = 0.001"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVi1y_mVTWMt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6922e5da-e319-49da-bb13-a462fa5da9c6"},"source":["methods = ['fixed','best_radius','best_radius_ncorr']\n","method = methods[1]\n","print('Method : %s' %method)\n","n_iter = 10\n","if method == 'fixed':\n","  Ncorr = 10\n","  rad = 0.3\n","  rad_test = [rad]\n","  rad , avg , all_accs_per_rad = knn.get_best_krad_param_angles(n_iter,Ncorr,\n","                                                                    n_systems,n_concepts,emb_dim,noise,n_epicentres,linearsep,sd,\n","                                                                    rad_test = rad_test)\n","  print(\"Ncorr : %s - Radius : %.2f - avg : %.2f\" %(Ncorr,rad,avg))\n","  #For run and dict_params\n","  rad,avg, all_accs,params_tested = rad,avg, all_accs_per_rad,rad_test #In effect we just test the fixed param\n","elif method == 'best_radius':\n","  Ncorr = 10\n","  rad_test = np.concatenate((np.arange(0.01,0.1,0.01),np.arange(0.05,0.45,0.05)),axis = 0)\n","  rad_test =np.around(rad_test, decimals=2)\n","  best_rad, best_avg , all_accs_per_rad = knn.get_best_krad_param_angles(n_iter,Ncorr,\n","                                                                    n_systems,n_concepts,emb_dim,noise,n_epicentres,linearsep,sd,\n","                                                                    rad_test = rad_test)\n","  print(\"Ncorr : %s  - Best rad : %.2f - Accuracy : %2f\" %(Ncorr,best_rad, best_avg))\n","  #For run and dict_params\n","  rad,avg, all_accs,params_tested = best_rad,best_avg, all_accs_per_rad,rad_test\n","elif method == 'best_radius_ncorr':\n","  Ncorr_test = [3] #,5,10,13,15]\n","  rad_test = np.concatenate((np.arange(0.01,0.1,0.01),np.arange(0.05,0.45,0.05)),axis = 0)\n","  rad_test =np.around(rad_test, decimals=2)\n","  pairs = list(itertools.product(Ncorr_test,rad_test))\n","  best_Ncorr_krad, best_avg , all_accs_per_Ncorr_krad = knn.get_best_Ncorr_krad_params_dist(n_iter,\n","                                  n_systems,n_concepts,emb_dim,noise,n_epicentres,linearsep,sd,\n","                                  Ncorr_test,rad_test = rad_test)\n","  print(\"Best Ncorr / k : %s - Accuracy : %2f\" %(best_Ncorr_krad, best_avg))\n","  #For run and dict_params\n","  Ncorr, rad = best_Ncorr_krad \n","  avg, all_accs,params_tested = best_avg , all_accs_per_Ncorr_krad,pairs"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Iter 1 - Testing param : 0.25 ...'"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"IjhjxiNFaSDW","colab_type":"code","colab":{}},"source":["#=Define models / optimizer/losses\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","#==SETUP=====\n","template_ceil = '    Ceiling\\n    Accuracy 1: {0:.2f} 5: {1:.2f} 10: {2:.2f} Half: {3:.2f}\\n'\n","template_loss = '    Epoch {0} Loss | total: {1:.5g} | cycle: {2:.3g} | f_dist: {3:.3g} | g_dist: {4:.3g}'\n","template_acc = '{0} = | 1: {1:.2f} | 5: {2:.2f} | 10: {3:.2f} | half: {4:.2f}'\n","template_res = '{0} = | f1: {1:.2f} | fhalf: {2:.2f} | g1: {3:.2f} | ghalf: {4:.2f} | total: {5:.2f} cycle: {6:.2f} | f_dist: {7:.2f} | g_dist: {8:.2f} '\n","n_batch = int(np.ceil(n_concepts / batch_size))\n","#==Set folders to save results and models===\n","fp_save_runs,fp_save_models = utils.create_save_folder(fp_save,override,n_systems,n_concepts,noise,emb_dim,\n","                                                 n_epicentres,linearsep,max_restart,max_epoch,batch_size,name = experiment_name)\n","print(fp_save_runs)\n","#===Get seed lists\n","sd_run_list = list(np.random.choice(10000, n_runs, replace=False))\n","#===Save params\n","experiment_id = uuid.uuid4()\n","dict_params = {'experiment_name':experiment_name,'experiment_id':experiment_id,'n_systems':n_systems,'n_concepts':n_concepts,'noise':noise,'emb_dim':emb_dim,'n_epicentres':n_epicentres,'linearsep':linearsep,\n","               'n_runs':n_runs,'max_restart':max_restart,'max_epoch':max_epoch,'batch_size':batch_size,'gmm_scale':gmm_scale,'loss_distr_scale':loss_distr_scale,\n","               'loss_cycle_scale':loss_cycle_scale,'lr':lr,'Ncorr':Ncorr,'rad':rad, 'avg': avg , 'params_tested' : params_tested ,'all_accs':all_accs}\n","with open(str(fp_save_runs)+'/dict_params.pickle', 'wb') as handle:\n","  pickle.dump(dict_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","display(pd.DataFrame([dict_params]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Vxvhq7LDOSD","colab_type":"text"},"source":["# Prepare synthetic data"]},{"cell_type":"code","metadata":{"id":"zQLWulxnDOSD","colab_type":"code","colab":{}},"source":["A,B,X_A,X_B,B_shuff,B_idx_map = emb.get_data(n_systems,n_concepts,emb_dim,noise,n_epicentres,linearsep,sd,bplot=True)\n","# Determine ceiling performance.\n","acc_1, acc_5, acc_10, acc_half = utils.mapping_accuracy(X_A, X_B)\n","print(template_ceil.format(acc_1, acc_5, acc_10, acc_half))\n","#Print distance matrix local faetures\n","start = time.time()\n","D_XY = knn.get_DXY_angles(A,B_shuff,radius=rad,bplot = True)\n","X_pretrain,Y_pretrain, matches = knn.get_corres_data(A,B_shuff,D_XY,Ncorr)\n","acc , mat = knn.check_corresp(matches,B_idx_map)\n","print(acc)\n","print(mat)\n","print(X_pretrain.shape,Y_pretrain.shape)\n","end = time.time()\n","print(\"Took : %.2f s\"%(end-start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecynlhoeBPIN","colab_type":"code","colab":{}},"source":["def pretrain(model_f,model_g,optimizer,x_pretrain, y_pretrain):\n","      with tf.GradientTape() as tape_pretrain_f:\n","        f_x = model_f(x_pretrain)\n","        corr_loss_f = tf.reduce_mean(tf.keras.losses.MSE(f_x, y_pretrain))\n","      gradients_pretrain_f = tape_pretrain_f.gradient(corr_loss_f, model_f.trainable_variables)\n","      with tf.GradientTape() as tape_pretrain_g:\n","        g_y = model_g(y_pretrain)\n","        corr_loss_g = tf.reduce_mean(tf.keras.losses.MSE(g_y, x_pretrain))\n","      gradients_pretrain_g = tape_pretrain_g.gradient(corr_loss_g, model_g.trainable_variables)\n","      # Combine gradients.\n","      optimizer.apply_gradients(zip(gradients_pretrain_f, model_f.trainable_variables))\n","      optimizer.apply_gradients(zip(gradients_pretrain_g, model_g.trainable_variables))\n","      return corr_loss_f, corr_loss_g\n","\n","def train_batch_step(model_f,model_g,optimizer,x_batch, y_batch, loss_cycle_scale):\n","        with tf.GradientTape(persistent=True) as tape_cycle:\n","            x_c = model_g(model_f(x_batch))\n","            y_c = model_f(model_g(y_batch))\n","            cycle_loss = loss_cycle_scale * losses.flex_cycle_loss([x_batch,y_batch],[x_c,y_c], norm_type = 'l2')\n","        gradients_cycle_f = tape_cycle.gradient(cycle_loss, model_f.trainable_variables)\n","        gradients_cycle_g = tape_cycle.gradient(cycle_loss, model_g.trainable_variables)\n","        del tape_cycle\n","        optimizer.apply_gradients(zip(gradients_cycle_f, model_f.trainable_variables))\n","        optimizer.apply_gradients(zip(gradients_cycle_g, model_g.trainable_variables))\n","        return cycle_loss\n","    \n","def train_full_step(model_f,model_g,optimizer,x_all, y_all, loss_distr_scale,gmm_scale):\n","        k = 5\n","        beta = 0.1\n","        loss_knn_scale = 1\n","        with tf.GradientTape() as tape_f0:\n","            f_x = model_f(x_all)\n","            dist_loss_f = loss_distr_scale * losses.distribution_loss(y_all, f_x, gmm_scale)\n","            knn_loss_f = 0 #loss_knn_scale * knn_loss_calcX(y_all,f_x,k,beta)\n","            tot_loss_f = dist_loss_f + knn_loss_f\n","        gradients_target_fx = tape_f0.gradient(tot_loss_f, model_f.trainable_variables)\n","        with tf.GradientTape() as tape_g0:\n","            g_y = model_g(y_all)\n","            dist_loss_g = loss_distr_scale * losses.distribution_loss(x_all, g_y, gmm_scale)\n","            knn_loss_g = 0 #loss_knn_scale * knn_loss_calcX(x_all,g_y,k,beta)\n","            tot_loss_g = dist_loss_g + knn_loss_g\n","        gradients_target_gy = tape_g0.gradient(tot_loss_g, model_g.trainable_variables)\n","        # Combine gradients.\n","        optimizer.apply_gradients(zip(gradients_target_fx, model_f.trainable_variables))\n","        optimizer.apply_gradients(zip(gradients_target_gy, model_g.trainable_variables))\n","        return dist_loss_f, dist_loss_g , knn_loss_f , knn_loss_g"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RY52zUq7DOSL","colab_type":"code","colab":{}},"source":["def train_models_one_restart(i_run,i_restart,sd_restart,x,y,y_idx_map,model_f,model_g,max_epoch,n_batch,x_0,x_1):\n","    #LOOP THROUGH EPOCHS\n","    epochs_dict_list = []\n","    for i_epoch in range(max_epoch):\n","        for x_batch, y_batch in utils.get_batch(x, y, batch_size):        \n","            #cycle_loss = train.train_batch_step(model_f,model_g,optimizer,x_batch, y_batch, loss_cycle_scale)\n","            cycle_loss = train_batch_step(model_f,model_g,optimizer,x_batch, y_batch, loss_cycle_scale)\n","        #dist_loss_f, dist_loss_g, knn_loss = train.train_full_step(model_f,model_g,optimizer,x, y,loss_distr_scale,gmm_scale)\n","        dist_loss_f, dist_loss_g, knn_loss_f , knn_loss_g = train_full_step(model_f,model_g,optimizer,x, y,loss_distr_scale,gmm_scale)\n","        knn_loss = knn_loss_f + knn_loss_g\n","        loss_total = dist_loss_f + dist_loss_g + cycle_loss + knn_loss\n","        # Project concept using current models\n","        f_x = model_f(x).numpy()\n","        g_y = model_g(y).numpy()\n","        #Get accuracies\n","        if y_idx_map is not None:\n","            acc_f1, acc_f5, acc_f10, acc_fhalf = utils.mapping_accuracy(f_x, y.numpy()[y_idx_map])\n","            acc_g1, acc_g5, acc_g10, acc_ghalf = utils.mapping_accuracy(g_y[y_idx_map], x.numpy())\n","        #Record all data in dict. \n","        dict_entry = {'experiment_name':experiment_name,'experiment_id':experiment_id,'run':i_run,'restart':i_restart,'sd_restart':sd_restart,'epoch':i_epoch,'loss_total':loss_total.numpy(),\n","                      'cycle_loss':cycle_loss.numpy(),'dist_loss_f':dist_loss_f.numpy(),'dist_loss_g':dist_loss_g.numpy(),\n","                     'acc_f1':acc_f1, 'acc_f5':acc_f5, 'acc_f10':acc_f10, 'acc_fhalf':acc_fhalf,\n","                     'acc_g1':acc_g1, 'acc_g5':acc_g5, 'acc_g10':acc_g10, 'acc_ghalf':acc_ghalf}\n","        epochs_dict_list.append(dict_entry)\n","        #Print current results\n","        if i_epoch % max_epoch ==0 or i_epoch==max_epoch-1:\n","            print(template_res.format(i_epoch,acc_f1,acc_fhalf,acc_g1,acc_ghalf,loss_total,cycle_loss,dist_loss_f,dist_loss_g))\n","            #print('Epoch %d -'%i_epoch,template_acc.format('f(x)', acc_f1, acc_f5, acc_f10, acc_fhalf),template_acc.format('g(x)', acc_g1, acc_g5, acc_g10, acc_ghalf)) \n","    #Plot final epoch\n","    #utils.plot_systems_results(x,y,f_x,g_y,x_0,x_1)\n","    return model_f,model_g,epochs_dict_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lg5dAk6DOSO","colab_type":"code","colab":{}},"source":["all_res = []\n","summary_res = []\n","for i_run in range(n_runs):\n","    print('===============================')\n","    print('======Run {0}======'.format(i_run))\n","    print('===============================')\n","    sd_run = utils.set_seed_np_tf(sd_run_list[i_run])\n","    print(\"Seed : \", sd_run)\n","    sd_restart_list = list(np.random.choice(10000, max_restart, replace=False))\n","    run_folder = fp_save_models+'/run'+str(i_run)\n","    utils.create_folder(run_folder+ '/final') #Same as last restart\n","    #==Get data \n","    A,B,X_A,X_B,B_shuff,B_idx_map = emb.get_data(n_systems,n_concepts,emb_dim,noise,n_epicentres,linearsep,sd_run,bplot=True)\n","    ceil_acc_1, ceil_acc_5, ceil_acc_10, ceil_acc_half = utils.mapping_accuracy(X_A, X_B)\n","    print(template_ceil.format(ceil_acc_1, ceil_acc_5, ceil_acc_10, ceil_acc_half))\n","    thresh_acc = ceil_acc_1 * 0.9\n","    print(\"    Threshold accuracy : %.2f\" %thresh_acc )\n","    x, y , y_idx_map = A , B_shuff , B_idx_map\n","    x = tf.convert_to_tensor(x, dtype = np.float32())\n","    y = tf.convert_to_tensor(y, dtype = np.float32())\n","    #==PRETRAIN - Get data\n","    D_xy = knn.get_DXY_angles(x,y,radius=rad,bplot = False)\n","    x_pretrain, y_pretrain, matches = knn.get_corres_data(x,y,D_xy,Ncorr)\n","    matches_acc , matches_mat = knn.check_corresp(matches,y_idx_map)\n","    #Save data\n","    with open(str(run_folder)+'/x.pickle', 'wb') as handle:\n","        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    with open(str(run_folder)+'/y.pickle', 'wb') as handle:\n","        pickle.dump(y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    #print(x.shape,y.shape)\n","    #==Loop through restarts\n","    #=init best\n","    loss_best = np.inf\n","    best_model_f = models.MLP(emb_dim, emb_dim)\n","    best_model_g = models.MLP(emb_dim, emb_dim)\n","    dist_loss_g_best = np.inf\n","    dist_loss_f_best = np.inf\n","    #=Loop\n","    last_restarts = []\n","    restarts_dict_list = []\n","    acc_f1, acc_g1 = 0,0\n","    last_restart = -1\n","    for i_restart in tqdm(range(max_restart)):\n","        sd_restart = utils.set_seed_np_tf(sd_restart_list[i_restart])\n","        print('======Restart {0}======'.format(i_restart))\n","        #==init models\n","        thresh = 0 #if 0, always a new model except if use_best_model=True. \n","        force_best_model = (i_restart>=max_restart-10) #if True, use best mode, else, random depends on thresh\n","        model_f,new_f = models.init_model(force_best_model,thresh,i_restart,sd_restart,emb_dim,best_model_f)\n","        model_g,new_g = models.init_model(force_best_model,thresh,i_restart,sd_restart,emb_dim,best_model_g)\n","        if new_f : print('    New model_f')\n","        if new_g : print('    New model_g')\n","        #==Pretrain\n","        for i_epoch in range(max_epoch):\n","          corr_loss_f, corr_loss_g = pretrain(model_f,model_g,optimizer,x_pretrain, y_pretrain)\n","        # f_x = model_f(x).numpy()\n","        # g_y = model_g(y).numpy()\n","        # acc_f1, acc_f5, acc_f10, acc_fhalf = utils.mapping_accuracy(f_x, y.numpy()[y_idx_map])\n","        # print(\"After pretraining...\")\n","        # print(template_acc.format(-1,acc_f1,acc_f5,acc_f10,acc_fhalf))\n","        # utils.plot_systems_results(x,y,f_x,g_y,X_A,X_B)\n","        #==Train\n","        model_f,model_g,epochs_dict_list = train_models_one_restart(i_run,i_restart,sd_restart,\n","                                                                    x,y,y_idx_map,model_f,model_g,max_epoch,n_batch,X_A,X_B)\n","        #Get final losses and accuracies\n","        last_epoch_dict = epochs_dict_list[-1]\n","        loss_total, cycle_loss, dist_loss_f, dist_loss_g = utils.get_losses_from_dict(last_epoch_dict)\n","        acc_f1, acc_f5, acc_f10, acc_fhalf,acc_g1, acc_g5, acc_g10, acc_ghalf = utils.get_acc_from_dict(last_epoch_dict)\n","        #Save best model\n","        best_f,best_g = False,False\n","        if dist_loss_f < dist_loss_f_best:\n","                dist_loss_f_best = dist_loss_f\n","                best_model_f = model_f\n","                best_f = True\n","                print('    Beat best model_f.')\n","        if dist_loss_g < dist_loss_g_best:\n","            dist_loss_g_best = dist_loss_g\n","            best_model_g = model_g\n","            best_g = True\n","            print('    Beat best model_g.')  \n","        #Add and Store data\n","        for i in range(len(epochs_dict_list)):\n","            epochs_dict_list[i]['new_f']=new_f\n","            epochs_dict_list[i]['new_g']=new_g\n","            epochs_dict_list[i]['best_f']=best_f\n","            epochs_dict_list[i]['best_g']=best_g\n","        restarts_dict_list.append(epochs_dict_list)\n","        #====Save restart model\n","        restart_folder = run_folder + '/restart' + str(i_restart)\n","        utils.create_folder(restart_folder)\n","        utils.save_models(model_f,model_g,restart_folder)\n","        #====Breaking loop\n","        if (acc_f1>thresh_acc and acc_g1>thresh_acc) and last_restart==-1: #last_restart = -1 if this does not happen\n","            last_restart = i_restart\n","            print(\"%d restarts necessary\" %(i_restart+1))\n","            #break\n","    if last_restart==-1: \n","      last_restart = max_restart\n","    all_dict_entries = utils.flatten_list_of_list(restarts_dict_list) \n","    all_res.append(all_dict_entries)\n","    #STORE FINAL RESULTS\n","    summary_dict = {'experiment_name':experiment_name,'experiment_id':experiment_id,'sd_run':sd_run,'last_restart': last_restart,'loss_total':loss_total,'cycle_loss':cycle_loss,\n","                    'dist_loss_f':dist_loss_f,'dist_loss_g':dist_loss_g,\n","                    'acc_f1':acc_f1, 'acc_f5':acc_f5, 'acc_f10':acc_f10, 'acc_fhalf':acc_fhalf,\n","                    'acc_g1':acc_g1, 'acc_g5':acc_g5, 'acc_g10':acc_g10, 'acc_ghalf':acc_ghalf,\n","                    'ceil_acc_1':ceil_acc_1, 'ceil_acc_5':ceil_acc_5, 'ceil_acc_10':ceil_acc_10, 'ceil_acc_half':ceil_acc_half,\n","                    'matches_acc':matches_acc,'matches_mat':matches_mat}\n","    summary_res.append(summary_dict)\n","    print(print('======Run Summary======'))\n","    print(display(pd.DataFrame([summary_dict])))\n","    #=====SAVE RUN MODEL====\n","    utils.save_models(model_f,model_g,run_folder+ '/final')\n","print('=========================')\n","print('=======FINAL SUMMARY=====')\n","print('=========================')\n","df_summary = pd.DataFrame(summary_res)\n","display(df_summary)\n","#Save results\n","with open(str(fp_save_runs)+'/all_res.pickle', 'wb') as handle:\n","    pickle.dump(all_res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","with open(str(fp_save_runs)+'/summary_res.pickle', 'wb') as handle:\n","    pickle.dump(summary_res, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3E_JBiNLYU5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}